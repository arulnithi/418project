{
  "name": "ParaPy",
  "tagline": "It's like CUDA, but for Python",
  "body": "# Checkpoint\r\n\r\n### Work So Far\r\n**Read and understood Python libraries which support processing of Python syntax grammar to parse simple functions**\r\n\r\nOur current program now allows us to parse simple python code using the python ast and inspect modules. We are able to break down the function into its full syntax tree. However, due to the limitations of the ast module, and the innate differences between Python and C++, the conversion process is not straightforward. One example is that the ast module does not recognize parentheses, thus mathematical instructions that require operations inside parentheses to be completed first will not be translated correctly. \r\n\r\nTo overcome this, we found a astor module, which is a newer module with additional functionality. We need to work integrating this into our current program.\r\n\r\n**Started to write the formatter class to convert parsed Python code to C++ code**\r\n\r\nWe started work on the formatter class to convert the parsed function into C++, CUDA and OpenMp code. We have finishing handling the conversion to C++ code and have verified that it compiles. Our current program is able to handle calls to the math library, arithmetic operations and simple assignments. \r\n\r\n**Configured and Understood the semantics of CUDA**\r\n\r\nWe have also read through various examples of CUDA functions as well as understood what is needed for CUDA to compile. This will help us as we try to format the parsed code into a CUDA file.\r\n\r\n\r\n\r\n### Currently Working On\r\n**Generate CUDA kernel code without errors given Python function which meets basic requirements (only arithmetic operations and no function calls within a function)**\r\n\r\nWe are currently working on getting our parser to work on a broader set of functions. Since we want to generate CUDA kernel code for Python functions which have high arithmetic intensity and scope for parallel implementation. An example of such a function would be to mapping a computationally intensive function onto a large array. We are working towards successfully generating C++ code and CUDA kernel code for such a function by this week.\r\n\r\n\r\n### Deliverable\r\n**Compile module on the Mandelbrot function and compare speedup to standard Python implementation**\r\n\r\nWe will be slightly behind our projected schedule for working on this item. However, we understand this to be our main achievement goal for this project. This is will really test the validity of what we have worked on in the previous weeks and we foresee having to go back and make many adjustments. Hence we believe that if we can achieve this target, we can start to tweak our project for better speed up, larger scope of functions and adding OpenMP functionality, which are our stretch goals.\r\n\r\n\r\n### New Schedule\r\n    April 4 - 10\r\n    -Read and understand Python libraries which support processing of Python syntax grammar\r\n    -Write the parser functions using the Python libraries as the first step for our module\r\n\r\n    April 11 - 17\r\n    -Write the parser functions using the Python libraries as the first step for our module\r\n    -Generate C++ code without errors given Python function which meets basic requirements (only arithmetic operations and function             calls within a function)\r\n    -Work on checkpoint report\r\n\r\n    April 19 - 24\r\n    -Generate CUDA kernel code without errors given Python function which mimics high arithmetic intensity and sufficient scope for parallelism [Ram]\r\n    -Tweak Mandelbrot function to satisfy our program restrictions [Arul]\r\n\r\n    April 25 - 30\r\n    -Generate CUDA kernel code correctly for the Mandelbrot function [Ram]\r\n    -Write Compiler class to compile module on the Mandelbrot function using Python FFI [Arul]\r\n\r\n    May 1 - 3\r\n    -Compare speedup to standard Python implementation [Ram]\r\n    -Produce graphs for comparison [Arul]\r\n\r\n    May 4 - 7\r\n    -Add functionality for higher control over block dimensions of CUDA kernel [Ram]\r\n    -Start working on integrating OpenMP code if times permits [Arul]\r\n    -Finalise the module compiler and fix any bugs [Ram]\r\n   \r\n\r\n\r\n# Proposal\r\n\r\n### Team Members\r\nArulnithi Sundaramoorthy (arulnits)\r\n\r\nRam Verma (ramv)\r\n\r\n\r\n### Summary\r\nWe are going to implement a Python module which takes Python code and translates it to C++ code targeting a CUDA compiler for the execution of the code.\r\n\r\n\r\n### Background\r\nPython is a widely used, general purpose, interpreted programming language whose code allows users to express concepts in fewer lines of code as compared to C++ or Java. However, Python suffers from being slow due to its Global Interpreter Lock (GIL) which does not allow multi-threading. \r\n\r\nTo combat this, there are many libraries which exist to allow multi-threading in Python such as processing which is a fork based process creation tool and Cython which allows calls to C libraries for parallelism. Moreover, ParaPy allows the user to type in C as part of a larger Python program to harness the power of GPUs. However, most of these tools are either restricted by the GIL or require some form of C/C++ code on top of Python code. \r\n\r\n____\r\nThus, we want to have a module which allows you to write standard Python syntax code and use just the methods of our module to harness the full potential of GPUs. \r\n___\r\n\r\n### The Challenge\r\nThere are many challenges associated with the project. We will have to correctly translate Python code to C++ CUDA code. This is a challenging task given the nature of both languages. Python is dynamically typed, meaning that there is no static type information available. We will require the user to confirm to certain restrictions ( not calling nested functions, using data of a singular primitive types, declaring type information before hand). \r\n\r\nAnother challenge is CUDAâ€™s distinction between device memory and host memory, and also the availability of shared memory to achieve even greater speed up. Python only has one type of memory, and hence we need to identify when and how such memory needs to be allocated. We plan to take care of device and host memory transfers, and disregard potential speedups achieved from shared memory optimizations. \r\n\r\n### Resources\r\nWe are planning to use several built in modules that allow for source code manipulation.The _Inspect module_ provides several useful functions to help get information about live objects such as classes and functions and retrieve their source code. The _AST module_ then allows us to generate an abstract syntax tree from this Python source code. We would then proceed to generate compliable C++ CUDA code using the above information.\r\n\r\n### Goals and Deliverables\r\n####*PLAN TO ACHIEVE*\r\nWe plan to have a completed Python module compiler which generates and compiles CUDA code for written Python code. The compiler should support arithmetic and boolean operations as well as libraries common to Python and C++.  \r\n####*HOPE TO ACHIEVE*\r\nIf we are able to complete the module compiler for CUDA, we plan to introduce finer level of control over the kernel calls in the compiler. We would also like to look into support for CPU parallelism by generating C++ code which uses OpenMP.\r\n\r\n\r\n### Platform Choice\r\nWe chose Python due to its simplicity and availability of multiple libraries for source code parsing. By having a module such as ParaPy, we hope to combine the simplicity of Python with the speed of C++.\r\n\r\nWe will be using the GHC 5000 machines with NVIDIA GPUs for design verification and testing since the NVIDIA compiler (nvcc) has already been set up on the machines.\r\n\r\n### Schedule\r\n    April 4th - 10th\r\n    -Read and understand Python libraries which support processing of Python syntax grammar\r\n    -Write the parser functions using the Python libraries as the first step for our module\r\n\r\n    April 11th - 17th\r\n    -Generate CUDA kernel code without errors given Python function which meets basic requirements \r\n    -Basic requirements include only arithmetic operations and no function calls within a function\r\n    -Work on checkpoint report\r\n\r\n    April 18th - 24th\r\n    -Compile module on the Mandelbrot function and compare speedup to standard Python implementation\r\n    -Produce graphs for comparison \r\n\r\n    April 25th - 30th\r\n    -Add more functionality to support generic library calls for the compiler\r\n    -Start working on generating OpenMP code if times permits\r\n\r\n    May 1st - 7th\r\n    -Finalize the module compiler and fix any bugs\r\n    -Work on report and website",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}